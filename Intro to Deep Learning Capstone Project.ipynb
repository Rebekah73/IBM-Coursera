{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "collapsed": true
            },
            "source": "# Final Project : Introduction to Deep Learning & Neural Networks with Keras\n\n##### Project Description\n\nIn this project, you will build a regression model using the Keras library to model the same data about concrete compressive strength that we used in labs 3."
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Requirement already satisfied: tensorflow in /opt/conda/envs/Python36/lib/python3.6/site-packages (1.13.1)\nRequirement already satisfied: absl-py>=0.1.6 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from tensorflow) (0.7.0)\nRequirement already satisfied: six>=1.10.0 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from tensorflow) (1.12.0)\nRequirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from tensorflow) (1.13.0)\nRequirement already satisfied: protobuf>=3.6.1 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from tensorflow) (3.6.1)\nRequirement already satisfied: keras-preprocessing>=1.0.5 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from tensorflow) (1.0.5)\nRequirement already satisfied: grpcio>=1.8.6 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from tensorflow) (1.16.1)\nRequirement already satisfied: keras-applications>=1.0.6 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from tensorflow) (1.0.6)\nRequirement already satisfied: gast>=0.2.0 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from tensorflow) (0.2.2)\nCollecting tensorboard<1.14.0,>=1.13.0 (from tensorflow)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/39/bdd75b08a6fba41f098b6cb091b9e8c7a80e1b4d679a581a0ccd17b10373/tensorboard-1.13.1-py3-none-any.whl (3.2MB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3.2MB 7.5MB/s eta 0:00:01\ufffd\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 3.0MB 7.5MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from tensorflow) (1.1.0)\nRequirement already satisfied: wheel>=0.26 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from tensorflow) (0.32.3)\nRequirement already satisfied: numpy>=1.13.3 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from tensorflow) (1.15.4)\nRequirement already satisfied: astor>=0.6.0 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from tensorflow) (0.7.1)\nRequirement already satisfied: setuptools in /opt/conda/envs/Python36/lib/python3.6/site-packages (from protobuf>=3.6.1->tensorflow) (40.8.0)\nRequirement already satisfied: h5py in /opt/conda/envs/Python36/lib/python3.6/site-packages (from keras-applications>=1.0.6->tensorflow) (2.9.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow) (3.0.1)\nRequirement already satisfied: werkzeug>=0.11.15 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow) (0.14.1)\nInstalling collected packages: tensorboard\nSuccessfully installed tensorboard-1.13.1\n"
                }
            ],
            "source": "# Installing Tensorflow\n\n!pip install tensorflow"
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Requirement already satisfied: keras in /opt/conda/envs/Python36/lib/python3.6/site-packages (2.2.4)\nRequirement already satisfied: numpy>=1.9.1 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from keras) (1.15.4)\nRequirement already satisfied: scipy>=0.14 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from keras) (1.2.0)\nRequirement already satisfied: six>=1.9.0 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from keras) (1.12.0)\nRequirement already satisfied: pyyaml in /opt/conda/envs/Python36/lib/python3.6/site-packages (from keras) (3.13)\nRequirement already satisfied: h5py in /opt/conda/envs/Python36/lib/python3.6/site-packages (from keras) (2.9.0)\nRequirement already satisfied: keras_applications>=1.0.6 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from keras) (1.0.6)\nRequirement already satisfied: keras_preprocessing>=1.0.5 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from keras) (1.0.5)\n"
                }
            ],
            "source": "# And installing keras\n\n!pip install keras"
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "Using TensorFlow backend.\n"
                }
            ],
            "source": "# And making sure that I have what I need...\n\nimport keras"
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": "# Importing the packages I need to make this run....\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense"
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Cement</th>\n      <th>Blast Furnace Slag</th>\n      <th>Fly Ash</th>\n      <th>Water</th>\n      <th>Superplasticizer</th>\n      <th>Coarse Aggregate</th>\n      <th>Fine Aggregate</th>\n      <th>Age</th>\n      <th>Strength</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>540.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>162.0</td>\n      <td>2.5</td>\n      <td>1040.0</td>\n      <td>676.0</td>\n      <td>28</td>\n      <td>79.99</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>540.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>162.0</td>\n      <td>2.5</td>\n      <td>1055.0</td>\n      <td>676.0</td>\n      <td>28</td>\n      <td>61.89</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>332.5</td>\n      <td>142.5</td>\n      <td>0.0</td>\n      <td>228.0</td>\n      <td>0.0</td>\n      <td>932.0</td>\n      <td>594.0</td>\n      <td>270</td>\n      <td>40.27</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>332.5</td>\n      <td>142.5</td>\n      <td>0.0</td>\n      <td>228.0</td>\n      <td>0.0</td>\n      <td>932.0</td>\n      <td>594.0</td>\n      <td>365</td>\n      <td>41.05</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>198.6</td>\n      <td>132.4</td>\n      <td>0.0</td>\n      <td>192.0</td>\n      <td>0.0</td>\n      <td>978.4</td>\n      <td>825.5</td>\n      <td>360</td>\n      <td>44.30</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
                        "text/plain": "   Cement  Blast Furnace Slag  Fly Ash  Water  Superplasticizer  \\\n0   540.0                 0.0      0.0  162.0               2.5   \n1   540.0                 0.0      0.0  162.0               2.5   \n2   332.5               142.5      0.0  228.0               0.0   \n3   332.5               142.5      0.0  228.0               0.0   \n4   198.6               132.4      0.0  192.0               0.0   \n\n   Coarse Aggregate  Fine Aggregate  Age  Strength  \n0            1040.0           676.0   28     79.99  \n1            1055.0           676.0   28     61.89  \n2             932.0           594.0  270     40.27  \n3             932.0           594.0  365     41.05  \n4             978.4           825.5  360     44.30  "
                    },
                    "execution_count": 5,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "concrete_data = pd.read_csv('https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0101EN/labs/data/concrete_data.csv')\nconcrete_data.head()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# Part A : Building a baseline model\n\nUse the Keras library to build a neural network with the following:\n\n- One hidden layer of 10 nodes, and a ReLU activation function\n\n- Use the adam optimizer and the mean squared error as the loss function.\n\nPerform the following steps:\n\n1. Randomly split the data into a training and test sets by holding 30% of the data for testing. You can use the ```train_test_split``` helper function from Scikit-learn.\n\n2. Train the model on the training data using 50 epochs.\n\n3. Evaluate the model on the test data and compute the mean squared error between the predicted concrete strength and the actual concrete strength. You can use the mean_squared_error function from Scikit-learn.\n\n4. Repeat steps 1 - 3, 50 times, i.e., create a list of 50 mean squared errors.\n\n5. Report the mean and the standard deviation of the mean squared errors.\n\n"
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "['Cement',\n 'Blast Furnace Slag',\n 'Fly Ash',\n 'Water',\n 'Superplasticizer',\n 'Coarse Aggregate',\n 'Fine Aggregate',\n 'Age',\n 'Strength']"
                    },
                    "execution_count": 6,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "concrete_data.columns.tolist()"
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": "# Split the data into feature and response\n\nX = concrete_data[['Cement', 'Blast Furnace Slag', 'Fly Ash', 'Water', 'Superplasticizer', \n                   'Coarse Aggregate', 'Fine Aggregate', 'Age']]\ny = concrete_data['Strength']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3)\n\n"
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": "# Determining the input shape for our model\n\nn_cols = X_train.shape[1]"
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": "# Building our initial model\n\ndef regression_model_base():\n    # create model\n    model = Sequential()\n    model.add(Dense(10, activation='relu', input_shape=(n_cols,)))\n    model.add(Dense(1))\n    \n    # compile model\n    model.compile(optimizer='adam', loss='mean_squared_error')\n    return model"
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "WARNING:tensorflow:From /opt/conda/envs/Python36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\n"
                }
            ],
            "source": "model = regression_model_base()"
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "WARNING:tensorflow:From /opt/conda/envs/Python36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.cast instead.\n"
                },
                {
                    "data": {
                        "text/plain": "<keras.callbacks.History at 0x7f679e933c50>"
                    },
                    "execution_count": 11,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "model.fit(X_train, y_train, validation_split=0.3, epochs=50, verbose=False)"
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "The MSE for my initial model is 162.87771191772387\n"
                }
            ],
            "source": "predictions = model.predict(X_test)\n\nMSE = mean_squared_error(y_test, predictions)\n\nprint('The MSE for my initial model is {}'.format(MSE))"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Running this 50 times\nClearly the simplest way to do this is using a for loop to \n1. resplit the data, \n2. fit the model, \n3. make the predictions, and \n4. compute the MSE\n50 times, each time saving the computed MSE to a list."
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "0\n10\n20\n30\n40\n"
                }
            ],
            "source": "# creating a list to store the computed MSE values\nMSE_list = []\n\nfor i in range(50):\n    if i%10 == 0:\n        print(i)\n    \n    # Do a train_test_split of the data\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3)\n\n    # Create a model\n    model = regression_model_base()\n    # And fit it on my current training data, setting verbose = False to avoid 50*50 = 2500 lines of feedback\n    model.fit(X_train, y_train, validation_split=0.3, epochs=50, verbose=False)\n    \n    # Making predictions on my held out data\n    predictions = model.predict(X_test)\n\n    # And computing the MSE\n    MSE = mean_squared_error(y_test, predictions)\n    \n    # And appending it to my list of MSE values so far\n    MSE_list.append(MSE)"
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [],
            "source": "base_mean = np.mean(MSE_list)\nbase_std = np.std(MSE_list)"
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "The mean of the MSE values for models of this type is 477.0868\nThe standard deviation of the MSE values for models of this type is 439.1986\n"
                }
            ],
            "source": "print('The mean of the MSE values for models of this type is {}'.format(np.around(base_mean, 4)))\nprint('The standard deviation of the MSE values for models of this type is {}'.format(np.around(base_std, 4)))"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# Part B : Using a normalized version of the data\n\nUse the Keras library to build a neural network with the following:\n\n- One hidden layer of 10 nodes, and a ReLU activation function\n\n- Use the adam optimizer and the mean squared error as the loss function.\n\nPerform the following steps:\n\n1. Randomly split the data into a training and test sets by holding 30% of the data for testing. You can use the ```train_test_split``` helper function from Scikit-learn.\n\n2. Normalize your data.\n\n2. Train the model on the training data using 50 epochs.\n\n3. Evaluate the model on the test data and compute the mean squared error between the predicted concrete strength and the actual concrete strength. You can use the mean_squared_error function from Scikit-learn.\n\n4. Repeat steps 1 - 3, 50 times, i.e., create a list of 50 mean squared errors.\n\n5. Report the mean and the standard deviation of the mean squared errors.\n\n\n#### Observation:\n\nHere we can use essentially the same set up as above, with the additional step that we need to normalize our data *using the mean and standard deviation of our __training data__* before fitting our model or making predictions."
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [],
            "source": "def train_test_normalized(X, y, percent_test = 0.3):\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = percent_test)\n    \n    # Computing mean and standard devations for the training data\n    \n    X_train_mean = np.mean(X_train, axis = 0)\n    X_train_std = np.std(X_train, axis = 0)\n    \n    y_train_mean = np.mean(y_train)\n    y_train_std = np.std(y_train)\n    \n    # Normalizing the training data\n    \n    X_train_norm = (X_train - X_train_mean)/X_train_std\n    y_train_norm = (y_train - y_train_mean)/y_train_std\n    \n    # Normalizing the test data (using the mean and standard deviation from the training data!!!)\n    \n    X_test_norm = (X_test - X_train_mean)/X_train_std\n    y_test_norm = (y_test - y_train_mean)/y_train_std\n    \n    return X_train_norm, X_test_norm, y_train_norm, y_test_norm"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Now, we should be able to use the model set up that we created in part A, but I'll go ahead and pull it down here to be safe."
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [],
            "source": "# Building our second model\n\ndef regression_model_norm():\n    # create model\n    model = Sequential()\n    model.add(Dense(10, activation='relu', input_shape=(n_cols,)))\n    model.add(Dense(1))\n    \n    # compile model\n    model.compile(optimizer='adam', loss='mean_squared_error')\n    return model"
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [],
            "source": "# Setting up the model\n\nmodel = regression_model_norm()"
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [],
            "source": "# Obtaining a normalized train-test split of our data\n\nX_train, X_test, y_train, y_test = train_test_normalized(X, y)"
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "<keras.callbacks.History at 0x7f63ac55eb70>"
                    },
                    "execution_count": 20,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# Fitting the model\n\nmodel.fit(X_train, y_train, validation_split=0.3, epochs=50, verbose=False)"
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "The MSE for my second model is 0.3657333248752294\n"
                }
            ],
            "source": "# Checking our predictions\n\npredictions = model.predict(X_test)\n\nMSE = mean_squared_error(y_test, predictions)\n\nprint('The MSE for my second model is {}'.format(MSE))"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Running this 50 times\n\nAgain, the simplest way to do this is with a 50 cycle for loop, saving the calculated MSE to a list at each pass. This time our loop will:\n1. Call ```train_test_normalized``` to create a normalized split of our data,\n2. Call ```regression_model_norm``` to create a neural network model,\n3. Fit the model on our normalized training data,\n4. Make predictions on our test data, and\n5. Compute the MSE, saving the computed value to a list at each step\n\nFollowing this, we can compute the mean and standard deviation of the computed MSE values."
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "0\n10\n20\n30\n40\n"
                }
            ],
            "source": "normed_MSE_list = []\n\nfor i in range(50):\n    if i%10 == 0:\n        print(i)\n    \n    # Do a normalized train_test_split of the data\n    X_train, X_test, y_train, y_test = train_test_normalized(X, y)\n\n    # Create a model\n    model = regression_model_norm()\n    # And fit it on my current training data, setting verbose = False to avoid 50*50 = 2500 lines of feedback\n    model.fit(X_train, y_train, validation_split=0.3, epochs=50, verbose=False)\n    \n    # Making predictions on my held out data\n    predictions = model.predict(X_test)\n\n    # And computing the MSE\n    MSE = mean_squared_error(y_test, predictions)\n    \n    # And appending it to my list of MSE values so far\n    normed_MSE_list.append(MSE)"
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {},
            "outputs": [],
            "source": "base_mean = np.mean(normed_MSE_list)\nbase_std = np.std(normed_MSE_list)"
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "The mean of the MSE values for models of type B is 0.2932\nThe standard deviation of the MSE values for models of type B is 0.0478\n"
                }
            ],
            "source": "print('The mean of the MSE values for models of type B is {}'.format(np.around(base_mean, 4)))\nprint('The standard deviation of the MSE values for models of type B is {}'.format(np.around(base_std, 4)))"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## How does the mean of the mean squared errors compare to that from Step A?\n\nWe have the following data:\n\n| | Part A | Part B |\n|--|--------|-------|\n|mean of MSE|477.0868|0.2932|\n|stdev of MSE| 439.1986| 0.0478|\n\nFrom this we see that both the mean and the standard deviation of the mean squared errors are significantly reduced for Part B as compared to Part A. While some of this can probably be explained by a reduction in the size of the values taken by the ```strength``` variable, this is unlikely to be the full explanation."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# Part C : Increasing the number of epochs to 100\n\n**Note that this is identical to part B but with a longer fitting stage**\n\nUse the Keras library to build a neural network with the following:\n\n- One hidden layer of 10 nodes, and a ReLU activation function\n\n- Use the adam optimizer and the mean squared error as the loss function.\n\nPerform the following steps:\n\n1. Randomly split the data into a training and test sets by holding 30% of the data for testing. You can use the ```train_test_split``` helper function from Scikit-learn.\n\n2. Normalize your data.\n\n2. Train the model on the training data using 100 epochs.\n\n3. Evaluate the model on the test data and compute the mean squared error between the predicted concrete strength and the actual concrete strength. You can use the mean_squared_error function from Scikit-learn.\n\n4. Repeat steps 1 - 3, 50 times, i.e., create a list of 50 mean squared errors.\n\n5. Report the mean and the standard deviation of the mean squared errors.\n\n\n#### Observation:\n\nHere we can use essentially the same set up as above, with the additional step that we need to normalize our data *using the mean and standard deviation of our __training data__* before fitting our model or making predictions."
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {},
            "outputs": [],
            "source": "# Building our third model\n\ndef regression_model_epochs():\n    # create model\n    model = Sequential()\n    model.add(Dense(10, activation='relu', input_shape=(n_cols,)))\n    model.add(Dense(1))\n    \n    # compile model\n    model.compile(optimizer='adam', loss='mean_squared_error')\n    return model"
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {},
            "outputs": [],
            "source": "# Setting up the model\n\nmodel = regression_model_epochs()"
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "metadata": {},
            "outputs": [],
            "source": "# Obtaining a normalized train-test split of our data\n\nX_train, X_test, y_train, y_test = train_test_normalized(X, y)"
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "<keras.callbacks.History at 0x7f6382811d30>"
                    },
                    "execution_count": 28,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# Fitting the model (for 100 epochs this time)\n\nmodel.fit(X_train, y_train, validation_split=0.3, epochs=100, verbose=False)"
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "The MSE for my third model is 0.23535468624363026\n"
                }
            ],
            "source": "# Checking our predictions\n\npredictions = model.predict(X_test)\n\nMSE = mean_squared_error(y_test, predictions)\n\nprint('The MSE for my third model is {}'.format(MSE))"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Running this 50 times\n\nAgain, the simplest way to do this is with a 50 cycle for loop, saving the calculated MSE to a list at each pass. This time our loop will:\n1. Call ```train_test_normalized``` to create a normalized split of our data,\n2. Call ```regression_model_epochs``` to create a neural network model,\n3. Fit the model on our normalized training data for 100(!) epochs,\n4. Make predictions on our test data, and\n5. Compute the MSE, saving the computed value to a list at each step\n\nFollowing this, we can compute the mean and standard deviation of the computed MSE values."
        },
        {
            "cell_type": "code",
            "execution_count": 30,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "0\n10\n20\n30\n40\n"
                }
            ],
            "source": "epochs_MSE_list = []\n\nfor i in range(50):\n    if i%10 == 0:\n        print(i)\n    \n    # Do a normalized train_test_split of the data\n    X_train, X_test, y_train, y_test = train_test_normalized(X, y)\n\n    # Create a model\n    model = regression_model_epochs()\n    # And fit it on my current training data, setting verbose = False to avoid 100*50 = 5000 lines of feedback\n    model.fit(X_train, y_train, validation_split=0.3, epochs=100, verbose=False)\n    \n    # Making predictions on my held out data\n    predictions = model.predict(X_test)\n\n    # And computing the MSE\n    MSE = mean_squared_error(y_test, predictions)\n    \n    # And appending it to my list of MSE values so far\n    epochs_MSE_list.append(MSE)"
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "metadata": {},
            "outputs": [],
            "source": "base_mean = np.mean(epochs_MSE_list)\nbase_std = np.std(epochs_MSE_list)"
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "The mean of the MSE values for models of type C is 0.2205\nThe standard deviation of the MSE values for models of type C is 0.0474\n"
                }
            ],
            "source": "print('The mean of the MSE values for models of type C is {}'.format(np.around(base_mean, 4)))\nprint('The standard deviation of the MSE values for models of type C is {}'.format(np.around(base_std, 4)))"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## How does the mean of the mean squared errors compare to that from Step B?\n\nWe have the following data:\n\n| | Part B | Part C |\n|--|--------|-------|\n|mean of MSE|0.2932|0.2205|\n|stdev of MSE| 0.0478|0.0474|\n\nWe see here that both the mean and standard deviation of the mean squared errors are somewhat smaller in Part C (with a longer training period) than than are in Part B."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# Part D : Increasing the number of hidden layers to 3\n\nUse the Keras library to build a neural network with the following:\n\n- Three hidden layers of 10 nodes, and a ReLU activation function\n\n- Use the adam optimizer and the mean squared error as the loss function.\n\nPerform the following steps:\n\n1. Randomly split the data into a training and test sets by holding 30% of the data for testing. You can use the ```train_test_split``` helper function from Scikit-learn.\n\n2. Normalize your data.\n\n2. Train the model on the training data using 50 epochs.\n\n3. Evaluate the model on the test data and compute the mean squared error between the predicted concrete strength and the actual concrete strength. You can use the mean_squared_error function from Scikit-learn.\n\n4. Repeat steps 1 - 3, 50 times, i.e., create a list of 50 mean squared errors.\n\n5. Report the mean and the standard deviation of the mean squared errors.\n\n\n#### Observation:\n\nHere we can use essentially the same set up as above, with the additional step that we need to normalize our data *using the mean and standard deviation of our __training data__* before fitting our model or making predictions."
        },
        {
            "cell_type": "code",
            "execution_count": 33,
            "metadata": {},
            "outputs": [],
            "source": "# Building our final model\n\ndef regression_model_deeper():\n    # create model\n    model = Sequential()\n    model.add(Dense(10, activation='relu', input_shape=(n_cols,)))\n    model.add(Dense(10, activation = 'relu'))\n    model.add(Dense(10, activation = 'relu'))\n    model.add(Dense(1))\n    \n    # compile model\n    model.compile(optimizer='adam', loss='mean_squared_error')\n    return model"
        },
        {
            "cell_type": "code",
            "execution_count": 34,
            "metadata": {},
            "outputs": [],
            "source": "# Setting up the model\n\nmodel = regression_model_deeper()"
        },
        {
            "cell_type": "code",
            "execution_count": 35,
            "metadata": {},
            "outputs": [],
            "source": "# Obtaining a normalized train-test split of our data\n\nX_train, X_test, y_train, y_test = train_test_normalized(X, y)"
        },
        {
            "cell_type": "code",
            "execution_count": 36,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "<keras.callbacks.History at 0x7f6378608da0>"
                    },
                    "execution_count": 36,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# Fitting the model (back to 50 epochs this time)\n\nmodel.fit(X_train, y_train, validation_split=0.3, epochs=50, verbose=False)"
        },
        {
            "cell_type": "code",
            "execution_count": 37,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "The MSE for my fourth (and final) model is 0.22409158733847825\n"
                }
            ],
            "source": "# Checking our predictions\n\npredictions = model.predict(X_test)\n\nMSE = mean_squared_error(y_test, predictions)\n\nprint('The MSE for my fourth (and final) model is {}'.format(MSE))"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Running this 50 times\n\nAgain, the simplest way to do this is with a 50 cycle for loop, saving the calculated MSE to a list at each pass. This time our loop will:\n1. Call ```train_test_normalized``` to create a normalized split of our data,\n2. Call ```regression_model_deeper``` to create a neural network model,\n3. Fit the model on our normalized training data for 50 epochs,\n4. Make predictions on our test data, and\n5. Compute the MSE, saving the computed value to a list at each step\n\nFollowing this, we can compute the mean and standard deviation of the computed MSE values."
        },
        {
            "cell_type": "code",
            "execution_count": 38,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n"
                }
            ],
            "source": "deeper_MSE_list = []\n\nfor i in range(50):\n    \n    print(i)\n    \n    # Do a normalized train_test_split of the data\n    X_train, X_test, y_train, y_test = train_test_normalized(X, y)\n\n    # Create a model\n    model = regression_model_deeper()\n    # And fit it on my current training data, setting verbose = False to avoid 50*50 = 2500 lines of feedback\n    model.fit(X_train, y_train, validation_split=0.3, epochs=50, verbose=False)\n    \n    # Making predictions on my held out data\n    predictions = model.predict(X_test)\n\n    # And computing the MSE\n    MSE = mean_squared_error(y_test, predictions)\n    \n    # And appending it to my list of MSE values so far\n    deeper_MSE_list.append(MSE)"
        },
        {
            "cell_type": "code",
            "execution_count": 39,
            "metadata": {},
            "outputs": [],
            "source": "base_mean = np.mean(deeper_MSE_list)\nbase_std = np.std(deeper_MSE_list)\\"
        },
        {
            "cell_type": "code",
            "execution_count": 40,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "The mean of the MSE values for models of type D is 0.2319\nThe standard deviation of the MSE values for models of type D is 0.0354\n"
                }
            ],
            "source": "print('The mean of the MSE values for models of type D is {}'.format(np.around(base_mean, 4)))\nprint('The standard deviation of the MSE values for models of type D is {}'.format(np.around(base_std, 4)))"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## How does the mean of the mean squared errors compare to that from Step B?\n\nWe have the following data:\n\n| | Part B | Part C | Part D |\n|--|--------|-------|--------|\n|mean of MSE|0.2932|0.2205|0.2319|\n|stdev of MSE| 0.0478|0.0474|0.0354|\n\nFrom this, we see that the mean and standard deviation of the mean squared errors in step D are smaller than those in step B, and also smaller than those in step C. In other words, we don't yet seem to be overfitting (at least not in any more significant way than previously), and adding more layers appears to improve performance more than simply training for a greater number of epochs."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ""
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}